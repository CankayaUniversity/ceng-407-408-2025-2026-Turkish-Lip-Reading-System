{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "nzmlIAPUdjFY"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "import cv2\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from typing import List\n",
        "from matplotlib import pyplot as plt\n",
        "import imageio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGnkARRFt4P0"
      },
      "source": [
        "-----------------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "y0QDMF7Hnc1p"
      },
      "outputs": [],
      "source": [
        "vocab = [x for x in \"abcçdefgğhıijklmnoöpqrsştuüvwxyz'?!0123456789 \"] # Turkish and English alphabets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "hnZC8IG3t6tv"
      },
      "outputs": [],
      "source": [
        "char_to_num = {char: idx for idx, char in enumerate(vocab)}\n",
        "num_to_char = {idx: char for char, idx in char_to_num.items()}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fIdhVd1yuJPb",
        "outputId": "62b3b4f9-fc4c-4a7c-ecad-0d9ddee0b2f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary (char_to_num): {'a': 0, 'b': 1, 'c': 2, 'ç': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'ğ': 8, 'h': 9, 'ı': 10, 'i': 11, 'j': 12, 'k': 13, 'l': 14, 'm': 15, 'n': 16, 'o': 17, 'ö': 18, 'p': 19, 'q': 20, 'r': 21, 's': 22, 'ş': 23, 't': 24, 'u': 25, 'ü': 26, 'v': 27, 'w': 28, 'x': 29, 'y': 30, 'z': 31, \"'\": 32, '?': 33, '!': 34, '0': 35, '1': 36, '2': 37, '3': 38, '4': 39, '5': 40, '6': 41, '7': 42, '8': 43, '9': 44, ' ': 45}\n",
            "Inverse Vocabulary (num_to_char): {0: 'a', 1: 'b', 2: 'c', 3: 'ç', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'ğ', 9: 'h', 10: 'ı', 11: 'i', 12: 'j', 13: 'k', 14: 'l', 15: 'm', 16: 'n', 17: 'o', 18: 'ö', 19: 'p', 20: 'q', 21: 'r', 22: 's', 23: 'ş', 24: 't', 25: 'u', 26: 'ü', 27: 'v', 28: 'w', 29: 'x', 30: 'y', 31: 'z', 32: \"'\", 33: '?', 34: '!', 35: '0', 36: '1', 37: '2', 38: '3', 39: '4', 40: '5', 41: '6', 42: '7', 43: '8', 44: '9', 45: ' '}\n",
            "Vocabulary Size (size): 46\n",
            "'hello' = [9, 5, 14, 14, 17]\n",
            "Reconstruction = 'hello'\n"
          ]
        }
      ],
      "source": [
        "print(f\"Vocabulary (char_to_num): {char_to_num}\")\n",
        "print(f\"Inverse Vocabulary (num_to_char): {num_to_char}\")\n",
        "print(f\"Vocabulary Size (size): {len(vocab)}\")\n",
        "word = \"hello\" # Example\n",
        "\n",
        "index = [char_to_num.get(char) for char in word]\n",
        "print(f\"'{word}' = {index}\")\n",
        "\n",
        "returned_word = \"\".join([num_to_char[idx] for idx in index])\n",
        "print(f\"Reconstruction = '{returned_word}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "MZL-R0GvpDzo"
      },
      "outputs": [],
      "source": [
        "class LipReadingDataset(Dataset):\n",
        "  def __init__(self, data_paths: list[str], char_to_num: dict):\n",
        "    self.data_paths = data_paths\n",
        "    self.char_to_num = char_to_num\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data_paths)\n",
        "\n",
        "  def __getitem__(self, idx: int) -> tuple[torch.Tensor, torch.Tensor]:\n",
        "    video_path = self.data_paths[idx]\n",
        "    file_name = os.path.basename(video_path).split('.')[0]\n",
        "    alignment_path = os.path.join('data',f'{file_name}.align')\n",
        "\n",
        "    frames = self._load_video(video_path)\n",
        "    alignments = self._load_alignments(alignment_path)\n",
        "    tokens = [self.char_to_num[char] for char in alignments if char in self.char_to_num]\n",
        "\n",
        "    return frames, torch.tensor(tokens, dtype=torch.int)\n",
        "\n",
        "\n",
        "  # Frames\n",
        "  def _load_video(self, path: str) -> torch.Tensor:\n",
        "    cap = cv2.VideoCapture(path)\n",
        "    frames_raw = []\n",
        "\n",
        "    pix_sum = 0.0\n",
        "    pix_sq_sum = 0.0\n",
        "    num_pix = 0\n",
        "\n",
        "    while True:\n",
        "      ret, frame = cap.read()\n",
        "      if not ret:\n",
        "        break\n",
        "      frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "      mouth_roi = frame[190:236, 80:220] # Mouth area\n",
        "      frames_raw.append(mouth_roi)\n",
        "\n",
        "      frame_normalized = frame / 255.0\n",
        "      pix_sum += np.sum(frame_normalized)\n",
        "      pix_sq_sum += np.sum(frame_normalized ** 2)\n",
        "      num_pix += frame_normalized.size\n",
        "\n",
        "    cap.release()\n",
        "\n",
        "    if num_pix == 0:\n",
        "        return torch.empty(0) # Return empty tensor if no frames\n",
        "\n",
        "    mean = pix_sum / num_pix\n",
        "    std = np.sqrt(pix_sq_sum / num_pix - mean**2)\n",
        "\n",
        "    frames_normalized_list = []\n",
        "    for frame in frames_raw:\n",
        "      frame_normalized = (frame / 255.0 - mean) / std\n",
        "      frames_normalized_list.append(frame_normalized)\n",
        "\n",
        "    v_tensor = torch.tensor(np.array(frames_normalized_list), dtype=torch.float32)\n",
        "    return v_tensor.unsqueeze(0)\n",
        "\n",
        "  # Transcription files\n",
        "  def _load_alignments(self, align_path: str) -> torch.Tensor:\n",
        "    ext = os.path.splitext(align_path)[1]\n",
        "    clean_text=\"\"\n",
        "\n",
        "    if ext == '.align': # Similar to Grid Corpus\n",
        "      words = []\n",
        "      with open(align_path, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "        for line in lines:\n",
        "            line = line.strip().split()\n",
        "            if len(line) >= 3:\n",
        "              if line[-1] != 'sil':\n",
        "                  words.append(line[-1])\n",
        "      clean_text = \" \".join(words)\n",
        "\n",
        "    else: # .txt etc.\n",
        "      with open(align_path, 'r', encoding='utf-8') as f:\n",
        "        text = f.read()\n",
        "        clean_text = text.strip()\n",
        "\n",
        "    return clean_text.lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ca76b91",
        "outputId": "a2eb8f16-0bff-4bdc-98b2-fe59ccefe9fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found video files: ['/content/data/bbaf2n.mpg'] and ['/content/data/bbaf2n.align']\n"
          ]
        }
      ],
      "source": [
        "data_dir = '/content/data'\n",
        "video_files = [os.path.join(data_dir, f) for f in os.listdir(data_dir) if f.endswith('.mpg')]\n",
        "alignment_files = [os.path.join(data_dir, f) for f in os.listdir(data_dir) if f.endswith('.align')]\n",
        "\n",
        "print(f\"Found video files: {video_files} and {alignment_files}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "608b33bc"
      },
      "outputs": [],
      "source": [
        "lip_reading_dataset = LipReadingDataset(video_files, char_to_num)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
